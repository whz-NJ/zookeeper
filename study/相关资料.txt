写数据流程：
https://www.cnblogs.com/zhuchangwu/p/11619978.html
dataLogDir 如果没提供的话使用的则是dataDir。zookeeper的持久化都存储在这两个目录里。dataLogDir里是放到的顺序日志(WAL)。
而dataDir里放的是内存数据结构的snapshot，便于快速恢复。为了达到性能最大化，一般建议把dataDir和dataLogDir分到不同的磁盘上，
这样就可以充分利用磁盘顺序写的特性。
SyncRequestProcessor 写 WAL 事务日志
https://segmentfault.com/a/1190000018927058
ZooKeeper的每个节点维护者三个Zxid值，分别为：cZxid、mZxid、pZxid。
cZxid：节点创建时间 create
mZxid：节点最近一次修改时间 modify
pZxid：该节点的子节点列表最后一次被修改时的时间，子节点内容变更不会变更pZxid
7）ZXID：每次对Zookeeper的状态的改变都会产生一个zxid（ZooKeeperTransaction Id），zxid是全局有序的，如果zxid1小于zxid2，则zxid1在zxid2之前发生。


数据恢复过程：
https://blog.csdn.net/w1673492580/article/details/89217556
其实在上篇关于选举的博客中说过，集群在启动时首先做的是数据加载loadDataBase()，然后才是选举。数据加载流程大概如下

创建ZKDatabase，给定对应的快照目录与事务日志目录
从快照目录中加载最后的快照文件内容。（由于每次快照都会全量dump数据，所以最新的快照数据通常是最全的）。
通过比较事务ID（快照文件名、事务日志文件名中的后缀即最新事务ID），获取事务记录增量数据（有可能跨多个事务日志文件），迭代事务日志文件列表，到快照之后产生的事务日志，进行更新。（即更新增量数据）将事务日志同步到committedLog中，以便集群间快速同步

（只是加载了快照，它落后于逻辑日志，也就是说启动后，内存里的数据可能不是最新的，这样后面再getdata就是错误的，需要把快照落后部分恢复到内存）
https://blog.csdn.net/IAmListening/article/details/95197707
在$ZOOKEEPER_HOME/data和$ZOOKEEPER_HOME/datalog文件夹中, 有zookeeper的快照和增量日志, 根据快照和增量日志, 可以将zookeeper恢复到指定的状态
快照和日志文件都是可读的, 参考使用下述指令（应该是先用快照恢复，再用增量日志恢复）
# 查看Log
java -cp dist-maven/zookeeper-3.4.9.jar;lib/log4j-1.2.16.jar;lib/slf4j-log4j12-1.6.1.jar;lib/slf4j-api-1.6.1.jar org.apache.zookeeper.server.LogFormatter log.4115cad8c1
# 查看快照
java -cp dist-maven/zookeeper-3.4.9.jar;lib/log4j-1.2.16.jar;lib/slf4j-log4j12-1.6.1.jar;lib/slf4j-api-1.6.1.jar org.apache.zookeeper.server.SnapshotFormatter snapshot.4115cad8bf
https://www.jianshu.com/p/67a166c4c1ba
1.每一次事物日志写入完毕以后，Zookeeper都会检测一次是否需要写入到快照中的操作，理论上达到snapCount次数以后的事物日志就要触发快照的demp操作，但是考虑整体性能，Zookeeper并不是每一次都会执行demp，而是选择使用了过半随机的原则，即:
logCount > (snapCount /2 + randRoll)
3.完成内存数据库的初始化以后，就要读取快照文件，进行全量数据恢复了，这个时候会默认读取最多一百个最新的快照文件，然后从ZXID最大的快照文件开始，进行逐个解析，进行反序列化操作，然后生成DataTree和sessionWithTimeout，并且根据checkSum校验完整性，如果校验失败，会放弃这个快照文件，选择第二个ZXID最大的快照文件，继续解析，依次类推，如果读取到的最多一百个快照文件都失败了，那么就直接启动失败，如果有校验成功的，则使用该文件进行全量恢复。




ZooKeeper选举流程分析（FLE）
https://zhuanlan.zhihu.com/p/48418023
electionEpoch
https://www.jianshu.com/p/75e48405d678
Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数Server完成了和leader的状态同步以后，恢复模式就结束了。状态同步保证了leader和Server具有相同的系统状态。leader选举是保证分布式数据一致性的关键。
https://my.oschina.net/iioschina/blog/872978

https://blog.csdn.net/weixin_43843104/article/details/112216733
每个当选产生一个新的 Leader服务器，就会从这个 Leader服务器上取出其本地日志中最大事务的 ZXID，并从中读取epoch 值，然后加 1，以此作为新的 epoch，并将低 32 位从 0 开始计数。
Zxid（Transaction id）类似于 RDBMS 中的事务 ID，用于标识一次更新操作的 Proposal（提议）ID。为了保证顺序性，该 zkid 必须单调递增。
日志文件中文件名为log，后缀即为十六进制的ZXID

https://www.cnblogs.com/veblen/p/10992103.html


分布式系统协议Paxos、Raft和ZAB
https://zhuanlan.zhihu.com/p/147691282
Raft\ZAB共同点和区别
首先，二者都是通过选举一个 Leader 来简化复杂度，后续的工作都是由 Leader 来做。
投票的时候，二者都需要定义一个轮次
Raft 定义了 term 来表示选举轮次
ZooKeeper 定义了 electionEpoch 来表示
同步数据的时候，都希望选举出来的 Leader 至少包含之前全部已提交的日志。
那如何能包含之前的全部日志？我们可以通过判断 Leader 节点中日志的逻辑时间序列，包含越新、越多日志的节点，越有可能包含之前全部的已提交日志。对于两种协议：

Raft：term 大的优先，然后 entry 的 index 大的优先
ZooKeeper：peerEpoch 大的优先，然后 zxid 大的优先
ZooKeeper 有 2 个轮次，一个是选举轮次 electionEpoch，另一个是日志的轮次 peerEpoch（即表示这个日志是哪个轮次产生的）。而 Raft 则是只有一个轮次，相当于日志轮次和选举轮次共用了。

继续对比，二者的选举效率也不同：
Raft 中的每个节点在某个 term 轮次内只能投一次票，哪个 Candidate 先请求投票谁就可能先获得投票，这样就可能造成分区，即各个 Candidate 都没有收到过半的投票，Raft 通过 Candidate 设置不同的超时时间，来快速解决这个问题，使得先超时的Candidate（在其他人还未超时时）优先请求来获得过半投票。
ZooKeeper 中的每个节点，在某个 electionEpoch 轮次内，可以投多次票，只要遇到更大的票就更新，然后分发新的投票给所有人。这种情况下不存在分区现象，同时有利于选出含有更新更多的日志的 Server，但是选举时间理论上相对 Raft 要花费的多。

在一个节点启动后，如何加入一个集群（这里是说本来就在集群配置内的一个节点）：
Raft：比较简单，该节点启动后，会收到 Leader 的 AppendEntries RPC，在这个 RPC 里面包含 Leader 信息，可以直接识别。
ZooKeeper：启动后，会向所有的其他节点发送投票通知，然后收到其他节点的投票。该节点只需要判断上述投票是否过半，过半则可以确认 Leader。

关于 Leader 选举的触发：
首先集群启动的时候，二者肯定都要先进行选举。
如果选举完成后，发生了超时：
Raft：目前只是 Follower 在检测。如过 Follower 在倒计时时间内未收到 Leader 的心跳信息，则 Follower 转变成 Candidate，自增 term 发起新一轮的投票。
ZooKeeper：Leader 和 Follower 都有各自的检测超时方式，Leader 是检测是否过半 Follower 心跳回复了，Follower 检测 Leader 是否发送心跳了。一旦 Leader 检测失败，则 Leader 进入 Looking 状态，其他 Follower 过一段时间因收不到 Leader 心跳也会进入 Looking 状态，从而出发新的 Leader 选举。一旦 Follower 检测失败了，则该 Follower 进入 Looking 状态，此时 Leader 和其他 Follower 仍然保持良好，则该 Follower 仍然是去学习上述 Leader 的投票，而不是触发新一轮的 Leader 选举。

关于上一轮次 Leader 残存的数据怎么处理：
包括两种数据：
已过半复制的日志
未过半复制的日志
Raft：对于之前 term 的过半或未过半复制的日志采取的是保守的策略，全部判定为未提交，只有当前 term 的日志过半了，才会顺便将之前 term 的日志进行提交
ZooKeeper：采取激进的策略，对于所有过半还是未过半的日志都判定为提交，都将其应用到状态机中

Raft 的保守策略更多是因为 Raft 在 Leader 选举完成之后，没有同步更新过程来保持和 Leader 一致（在可以对外处理请求之前的这一同步过程）。而 ZooKeeper 是有该过程的。
在对正常请求的处理方式上，二者都是基本相同的，大致过程都是过半复制。

对于正常请求的消息顺序保证：
Raft：对请求先转换成 entry，复制时，也是按照 Leader 中 log 的顺序复制给 Follower 的，对 entry 的提交是按 index 进行顺序提交的，是可以保证顺序的
ZooKeeper：在提交议案的时候也是按顺序写入各个 Follower 对应在 Leader 中的队列，然后 Follower 必然是按照顺序来接收到议案的，对于议案的过半提交也都是一个个来进行的

